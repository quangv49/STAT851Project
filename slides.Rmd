---
title: "STAT 851 Project - Firth's method for bias reduction of MLEs"
author: "Quang Vuong, Nadia Enhaili"
date: "26/03/2023"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Outline

\begin{enumerate}
\item Motivation
\item Firth's method - general case
\item Basic example
\item Firth's method - GLM case
\item Real data example
\end{enumerate}

## Motivation

- MLEs are consistent and asymptotically unbiased, but there is still a shrinking bias term.

- 1D case:

\[U(\hat\theta) = U(\theta) + U'(c(\hat\theta))(\hat\theta - \theta)\]
\[\Rightarrow \mathbb E U'(c(\hat\theta))(\hat\theta - \theta) = 0.\]

- If \(U'\) is not constant, then \(\hat\theta\) is certainly biased.

## Motivation

- Notation:
\[\theta = \begin{pmatrix} \theta^1 \\ \vdots \\ \theta^p \end{pmatrix} \in \Omega\]
\[U_r(\theta) = \frac{\partial l(\theta)}{\partial \theta^r}, U_{rs}(\theta) = \frac{\partial^2 l(\theta)}{\partial \theta^r \partial \theta^s}\]
\[\kappa_{r,s} = \frac{1}{n}\mathbb E U_rU_s, \kappa_{rs} = \frac{1}{n}\mathbb E U_{rs}\]
\[\kappa_{r,s,t} = \frac{1}{n}\mathbb E U_rU_sU_t, \kappa_{r,st} = \frac{1}{n}\mathbb E U_rU_{st}\]

## Motivation

- The first term in the bias of \(\hat\theta\) has been studied.

\[\mathbb E(\hat{\theta}^r - \theta^r) = -\frac{\kappa^{r,s}\kappa^{t,u}(\kappa_{s,t,u} + \kappa_{s,tu})}{2n} + O\left(\frac{1}{n^{\frac{3}{2}}}\right) = \frac{1}{n}b_1^r(\theta) + O\left(\frac{1}{n^{\frac{3}{2}}}\right).\]

## Motivation

Example: Fit a GLM to data generated by the following model.

\[Y \sim Exp(\lambda)\]
\[\log(\mu) = \log(\frac{1}{\lambda}) = 8 + 5x\]

Predictor values \(x_i\) are predetermined, while \(Y_i\) is repeatedly simulated at \(x_i\) 10000 times.

## Motivation

\begin{figure}[!h]
\includegraphics[scale=0.5]{b0_simul.png}
\end{figure}

## Motivation

\begin{figure}[!h]
\includegraphics[scale=0.5]{b1_simul.png}
\end{figure}

## Firth's method

- Reduce the bias by using an adjusted score function.

\[U^*(\theta) = U(\theta) + A(\theta), A: \Omega \to \mathbb R^p\]

- Root is \(\theta^*\).

## Firth's method

- One can derive the bias of \(\theta^*\) when estimating \(\theta\)

\begin{align*}
\mathbb E((\theta^*)^r - \theta^r) &= -\frac{\kappa^{r,s}(\frac{1}{2}\kappa^{t,u}(\kappa_{s,t,u} + \kappa_{s,tu}) + \mathbb E  A^s(\theta))}{n} + O\left(\frac{1}{n^{\frac{3}{2}}}\right) \\
&= \frac{b_1^r(\theta) + \kappa^{r,s}\mathbb E A^s(\theta)}{n} + O\left(\frac{1}{n^{\frac{3}{2}}}\right).
\end{align*}

## Firth's method

- Suppose that:

\begin{align*}
&b_1^r(\theta) + \kappa^{r,s}\mathbb E A^s(\theta) + O\left(\frac{1}{\sqrt{n}}\right) = 0 \\
&\Leftrightarrow \mathbb E A^r(\theta) = -\kappa_{r,s}b_1^s(\theta) + O\left(\frac{1}{\sqrt{n}}\right) \\
&\Leftrightarrow \mathbb E A(\theta) = -\frac{1}{n}\mathcal I(\theta)b_1(\theta) + O\left(\frac{1}{\sqrt{n}}\right)
\end{align*}

- Then the first term of the bias is gone.

## Firth's method

- Firth (1993) suggests

\[A^{(O)}(\theta) = -\frac{1}{n}I(\theta)b_1(\theta),\]
or
\[A^{(E)}(\theta) = -\frac{1}{n}\mathcal I(\theta)b_1(\theta).\]

- The first one leads to more efficient (i.e. less variance) estimators.